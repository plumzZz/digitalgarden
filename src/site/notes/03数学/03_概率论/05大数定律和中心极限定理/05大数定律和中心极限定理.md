---
{"tags":[],"dg-publish":true,"permalink":"/03数学/03_概率论/05大数定律和中心极限定理/05大数定律和中心极限定理/","dgPassFrontmatter":true}
---

---
# 依概率收敛

设  $X_{1}, X_{2}, \dots , X_{n}, \dots$  是一个随机变量序列,  $a$  是一个常数. 如果对于任意给定的正数  $\epsilon$ , 有  $\lim_{n \to \infty} P\left\{\left|X_{n} - a\right| < \epsilon \right\} = 1$ ,
则称随机变量序列  $X_{1}, X_{2}, \dots , X_{n}, \dots$  依概率收敛于  $a$ , 记为  $X_{n} \xrightarrow{P} a$ 
**良哥解读**
(1)依概率收敛与高等数学中数列收敛定义类似,数列  $\{x_{n}\}$  收敛于  $a$ ,描述的是当  $n$  充分大时,  $x_{n}$  与  $a$  的距离  $\left|x_{n} - a\right|$  任意小;随机变量序列  $\{X_{n}\}$  依概率收敛于  $a$ ,描述的是当  $n$  充分大时,  $X_{n}$  与  $a$  的距离  $\left|X_{n} - a\right|$  任意小的概率为 1.
(2)依概率收敛还可以用  $\lim_{n \to \infty} P\left\{\left|X_{n} - a\right| < \epsilon \right\} = 1$  的逆事件形式表示:
$$
\lim_{n\to \infty}P\left\{\left|X_{n} - a\right|\geqslant \epsilon \right\} = 0.
$$
![Pasted image 20250916194626.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916194626.png)
![Pasted image 20250916194733.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916194733.png)



# 大数定律

## (一)切比雪夫大数定律
设  $X_{1}, X_{2}, \dots , X_{n}, \dots$  是由两两不相关(或**两两独立**)（可不同分布）的随机变量所构成的序列,分别具有**数学期望**  $E(X_{1}), E(X_{2}), \dots , E(X_{n}), \dots$  和**方差**  $D(X_{1}), D(X_{2}), \dots , D(X_{n}), \dots$  并且方差**有公共上界**,即存在正数  $M$ ,使得  $D(X_{n}) \leqslant M, n = 1,2 \dots$ ,则对于任意给定的正数  $\epsilon$ ,总有

$$
\lim_{n\to \infty}P\left\{\left|\frac{1}{n}\sum_{k = 1}^{n}X_{k} - \frac{1}{n}\sum_{k = 1}^{n}E(X_{k})\right|< \epsilon \right\} = 1~. \tag{一切尽意}
$$
![Pasted image 20250916195142.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916195142.png)
它从数学上严格证明了：当随机变量的观测次数足够多时，其样本均值会稳定在总体均值附近，为 “频率趋近于概率”“样本推断总体” 等统计思想提供了理论基础。

$E(X_{k})$并不是“一个常数的期望”，而是随机变量$X_{k}$的数学期望（均值）
**详细解释**
- $X_{k}$是一个随机变量，它的取值是具有随机性的（比如抛一次硬币，结果是正面或反面，对应随机变量取值为$1$或$0$，是随机的）。
- $E(X_{k})$是对随机变量$X_{k}$的“取值平均水平”的刻画：
  - 若$X_{k}$是离散型随机变量，$E(X_{k})=\sum_{x} x P(X_{k}=x)$（对所有可能取值$x$，乘以其概率后求和）；
  - 若$X_{k}$是连续型随机变量，$E(X_{k})=\int_{-\infty}^{+\infty} x f(x) dx$（$f(x)$是概率密度函数）。
与“**常数的期望**”的区别
常数$C$的期望就是它本身，即$E(C)=C$；而$E(X_{k})$是对**随机变量$X_{k}$**的平均取值的计算，$X_{k}$本身不是常数（除非$X_{k}$是“退化的随机变量”，即几乎必然取某个常数，但这是特殊情况）。
结合切比雪夫大数定律的语境
在$\frac{1}{n}\sum_{k = 1}^{n} E(X_{k})$中，每一个$E(X_{k})$是随机变量$X_{k}$的均值，然后对这$n$个“均值”求算术平均——切比雪夫大数定律要说明的是：**当$n\to\infty$时，随机变量的“样本均值”（$\frac{1}{n}\sum_{k = 1}^{n} X_{k}$）会依概率收敛到“均值的算术平均”（$\frac{1}{n}\sum_{k = 1}^{n} E(X_{k})$）**。
**独立同分布的切比雪夫大数定律**
设随机变量  $X_{1}, X_{2}, \dots , X_{n}, \dots$  相互独立,服从相同的分布,具有数学期望  $E(X_{n}) = \mu$  和方差  $D(X_{n}) = \sigma^{2} (n = 1,2 \dots)$  则对于任意给定的正数  $\epsilon$ ,总有  $\lim_{n \to \infty} P\left\{\left|\frac{1}{n} \sum_{k = 1}^{n} X_{k} - \mu \right| < \epsilon \right\} = 1$ ,即随机变量序列  $\overline{X_{n}} = \frac{1}{n} \sum_{k = 1}^{n} X_{k} \xrightarrow{P} \mu$ .
**切比雪夫不等式**
2. 切比雪夫不等式的核心逻辑
切比雪夫不等式是说：**对任意随机变量 $X$，若它的期望 $E(X)$、方差 $D(X)$ 存在，那么对于任意正数 $\varepsilon$，有 $P(|X - E(X)| < \varepsilon) \geq 1 - \frac{D(X)}{\varepsilon^2}$**。
简单讲：随机变量 $X$ 落在“期望 $E(X)$ 附近 $\varepsilon$ 范围内”的概率，至少是 $1 - \frac{方差}{\varepsilon^2}$。方差越小、$\varepsilon$ 越大，这个概率就越接近1（说明 $X$ 越集中在期望附近）。
3. 代入伯努利试验的场景
在伯努利试验中（比如重复抛硬币、重复做“成功/失败”的试验）：
- 样本均值 $\bar{X}_n$ 的期望 $E(\bar{X}_n) = p$（因为伯努利试验“成功”的概率是 $p$，样本均值的期望就是真实概率）。
- 样本均值 $\bar{X}_n$ 的方差 $D(\bar{X}_n) = \frac{p(1 - p)}{n}$（这是伯努利试验样本均值方差的公式，$n$ 是试验次数，$p(1 - p)$ 是单次试验的方差）。
4. 不等式的推导与意义
把 $X = \bar{X}_n$、$E(X) = p$、$D(X) = \frac{p(1 - p)}{n}$ 代入切比雪夫不等式，就得到：

$$
P\left( \left| \bar{X}_n - p \right| < \varepsilon \right) \geq 1 - \frac{D(\bar{X}_n)}{\varepsilon^2} = 1 - \frac{p(1 - p)}{n\varepsilon^2}
$$

它的意义是：**当试验次数 $n$ 很大时，样本中“成功”的频率 $\bar{X}_n$ 与真实概率 $p$ 的差距小于 $\varepsilon$ 的概率，至少是 $1 - \frac{p(1 - p)}{n\varepsilon^2}$**。而且 $n$ 越大，$\frac{p(1 - p)}{n\varepsilon^2}$ 越小，这个概率就越接近1——这正是“伯努利大数定律”的核心（频率随试验次数增加，会稳定在概率附近）。
## (三)辛钦大数定律
设随机变量  $X_{1}, X_{2}, \dots , X_{n}, \dots$  **相互独立**,服从**相同的分布**,具有数学期望  $E(X_{n}) = \mu (n = 1,2 \dots)$  则对于任意给定的正数  $\epsilon$ ,总有

$$
\lim_{n\to \infty}P\left\{\left|\frac{1}{n}\sum_{k = 1}^{n}X_{k} - \mu \right|< \epsilon \right\} = 1\frac{\mathrm{~\#~}}{\mathrm{~\#~}}\lim_{n\to \infty}P\left\{\left|\frac{1}{n}\sum_{k = 1}^{n}X_{k} - \mu \right|\geqslant \epsilon \right\} = 0~.
$$
辛钦大数定律是概率论中描述 “大量随机事件平均结果稳定性” 的核心定律之一，是连接 “随机变量的**理论期望**” 与 “实际观测平均值” 的桥梁，也是后续数理统计（如参数估计、假设检验）的理论基石，通过足够多的独立观测，其平均值会 “稳定” 在理论期望附近。
辛钦大数定律：**概念解析与考点拓展**
辛钦大数定律是概率论中**描述“大量随机事件平均结果稳定性”** 的核心定律之一，是连接“随机变量的理论期望”与“实际观测平均值”的桥梁，也是后续数理统计（如参数估计、假设检验）的理论基石。要理解它，需先从“为什么需要大数定律”切入——在现实中，我们无法观测随机变量的所有可能取值来计算精确期望，但辛钦大数定律告诉我们：**通过足够多的独立观测，其平均值会“稳定”在理论期望附近**。
**关键前提条件**（不可忽略的“门槛”）
辛钦大数定律的成立依赖两个核心条件，这是理解和应用的关键，也是考点高频陷阱：
- **条件1：独立同分布（i.i.d.）**  
  “独立”指每个随机变量的取值不影响其他变量（如每次掷骰子的结果互不干扰）；“同分布”指所有随机变量服从相同的概率分布（如每次掷的都是同一枚均匀骰子，服从相同的离散分布）。  
  *注意：若序列不满足“同分布”，即使独立且期望存在，也可能不满足辛钦大数定律（例如 $X_i$ 分别服从 $U(0,i)$，期望随 $i$ 变化，平均值无法稳定）。*
- **条件2：数学期望存在**  
  随机变量的期望必须有限（即 $E[|X_i|] < +\infty$，因为期望存在的充要条件是“绝对期望有限”）。若期望不存在（如柯西分布 $X_i \sim Cauchy(0,1)$，其期望发散），则平均值 $\bar{X}_n$ 不会收敛到任何固定值，定律不成立。
**直观含义**（通俗解释）
用“掷均匀骰子”举例：骰子的理论期望 $\mu = \frac{1+2+3+4+5+6}{6} = 3.5$。  
- 掷10次，平均值可能是3.2或3.8（波动较大）；  
- 掷1000次，平均值可能是3.48或3.52（波动缩小）；  
- 掷100000次，平均值会非常接近3.5，且“偏离3.5的概率”会随着次数增加趋近于0。  
这就是辛钦大数定律的直观意义：**大量独立同分布的随机观测，其平均值会“无限靠近”理论期望**。
考点2：利用辛钦大数定律解释“**实际应用**”
辛钦大数定律是“用样本均值估计总体期望”的理论依据，考试中常结合实际场景提问，如：  
- “为什么多次测量同一物体的长度，取平均值更准确？”  
- “为什么保险公司能通过大量保单计算保费（保费本质是期望损失）？”  
**答题核心：** 结合定律表述——“该场景下的观测值（如每次测量值、每张保单的损失）是**独立同分布**的随机变量，且**期望存在**（如物体真实长度、单张保单的期望损失）；根据辛钦大数定律，当观测次数足够多时，样本平均值会**依概率收敛**于理论期望，因此平均值是期望的‘稳定估计’”。
考点3：**依概率收敛的性质与辛钦定律结合**
辛钦定律的结论是“样本均值依概率收敛于期望”，考试中可能结合“依概率收敛的性质”（如四则运算封闭性）出题：  
**例题：** 设 $X_1, X_2, \dots, X_n, \dots$ 独立同分布，$E[X_i] = \mu$，$E[X_i^2] = \sigma^2 + \mu^2$（即方差存在）。证明：$\frac{1}{n} \sum_{i=1}^n X_i^2 \xrightarrow{P} \sigma^2 + \mu^2$。  
**证明思路：**  
1. 因 $X_i$ 独立同分布，故 $X_i^2$ 也**独立同分布**（独立变量的函数仍独立，同分布变量的函数仍同分布）；  
2. 已知 $E[X_i^2] = \sigma^2 + \mu^2 < +\infty$（**期望存在**）；  
3. 根据辛钦大数定律，对 $X_i^2$ 序列，有 $\frac{1}{n} \sum_{i=1}^n X_i^2 \xrightarrow{P} E[X_i^2] = \sigma^2 + \mu^2$。  
考点4：**辛钦大数定律与伯努利大数定律的关系**
需明确“伯努利大数定律是辛钦大数定律的特例”：  
- 伯努利试验中，设 $X_i$ 为“第 $i$ 次试验成功与否”（成功=1，失败=0），则 $X_i \sim Bernoulli(p)$，独立同分布，且 $E[X_i] = p$；  
- 根据辛钦大数定律，样本均值 $\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$（即“成功频率”）依概率收敛于 $p$（即“成功概率”），这正是伯努利大数定律的结论。  
**考试角度：** 可能要求“用辛钦大数定律证明伯努利大数定律”，本质是验证伯努利序列满足辛钦定律的两个条件（i.i.d. + 期望存在）。
## (四) 伯努利大数定律
设在每次实验中事件  $A$  发生的概率  $P(A) = p$  ,在  $n$  次独立重复实验（伯努利实验）中,事件  $A$  发生的频率为  $f_{n}(A)$  则对于任意正数  $\epsilon$  ,总有  $\lim_{n\to \infty}P\left\{\left|f_{n}(A) - p\right|< \epsilon \right\} = 1$  或  $\lim_{n\to \infty}P\left\{\left|f_{n}(A) - p\right|\geqslant \epsilon \right\} = 0$
1. **定律的直观含义**
在 $n$ 重伯努利试验中，设 $n_A$ 为“成功”发生的次数（即“频数”），则 $\frac{n_A}{n}$ 为“成功”的**频率**。  
伯努利大数定律的核心思想是：**当试验次数 $n$ 足够大时，“成功”的频率 $\frac{n_A}{n}$ 会“稳定地趋近于”成功的概率 $p$”**——也就是说，即使单次试验的结果是随机的（比如抛硬币，单次可能正面也可能反面），但大量重复后，频率会“围绕概率波动且越来越近”，这正是“概率是频率的稳定值”这一朴素认知的数学严格化。
2. **定律的数学表述**（两种常见形式）
伯努利大数定律有“弱大数定律”（依概率收敛）和“强大数定律”（以概率1收敛）两种表述，本科阶段重点考察**弱大数定律**：
#### （1）弱大数定律（依概率收敛形式）
设 $n_A$ 是 $n$ 重伯努利试验中“成功”的次数，对任意给定的正数 $\varepsilon$（无论多么小），有：  

$$
\lim_{n \to \infty} P\left( \left| \frac{n_A}{n} - p \right| < \varepsilon \right) = 1
$$
其含义是：当 $n$ 无限增大时，“频率与概率的偏差小于任意小的 $\varepsilon$”这一事件的概率趋近于1（即“几乎必然发生”）。
#### （2）强大数定律（以概率1收敛形式）
对 $n$ 重伯努利试验中的 $n_A$，有：  
$$
P\left( \lim_{n \to \infty} \frac{n_A}{n} = p \right) = 1
$$

其含义是：“频率趋近于概率”这一事件的**发生概率为1**（比“依概率收敛”更强——前者是“几乎一定发生”，后者是“发生的概率趋近于1”）。
**考点1：概念辨析题（基础必考点）**
核心考点
- 区分“频率与概率的关系”（易错点：“频率等于概率”“频率一定趋近于概率”等错误表述）；  
- 判断某场景是否适用伯努利大数定律（关键看是否满足“独立重复、只有两种结果、概率固定”）。
**典型例题**
例1：判断下列说法是否正确：  
（1）抛100次硬币，正面出现50次，说明正面概率是0.5；  
（2）抛硬币次数越多，正面频率“一定”越接近0.5；  
（3）某产品的次品率为0.01，现抽查1000件，次品数的频率“很可能”接近0.01。  
**解析**：  
（1）错误。频率是概率的“估计值”，而非“等于概率”——100次的频率0.5是对概率的近似，不能直接等同于概率；  错误的核心：混淆 “依概率收敛” 与 “必然收敛”，频率与概率的关联由大数定律确立，但大数定律的严格表述是 “频率依概率收敛于概率”，而非 “**一定趋近**（必然收敛）于概率”。
（2）错误。“趋近于”是“概率意义上的趋近”（依概率收敛），而非“必然趋近”——可能存在少数大 $n$ 时频率偏离概率的情况，但这种情况发生的概率趋近于0；  
（3）正确。抽查1000件是1000重伯努利试验（次品=成功，概率0.01），根据伯努利大数定律，频率会以大概率接近0.01。
**考点2**：**收敛性证明**
核心考点
- 利用“切比雪夫不等式”证明伯努利大数定律（理解证明逻辑，而非死记公式）；  
- 区分“依概率收敛”与“以概率1收敛”的表述差异。
典型例题
例2：利用切比雪夫不等式证明伯努利大数定律（弱形式）。  
**证明步骤**：  
1. 设 $X_1,X_2,\dots,X_n$ 为**独立同分布**的**伯努利变量**，$X_i \sim B(1,p)$，则 $n_A = \sum_{i=1}^n X_i$，且：  
   $E(n_A) = np$，$D(n_A) = np(1-p)$；  
2. 对 $\bar{X}_n = \frac{n_A}{n}$，计算期望与方差：  
   $E(\bar{X}_n) = p$，$D(\bar{X}_n) = \frac{p(1-p)}{n}$；  
3. 应用切比雪夫不等式：对任意 $\varepsilon > 0$，  
   $P\left( |\bar{X}_n - p| \geq \varepsilon \right) \leq \frac{D(\bar{X}_n)}{\varepsilon^2} = \frac{p(1-p)}{n\varepsilon^2}$；  
4. 令 $n \to \infty$，右边趋近于0，故：  
   $\lim_{n \to \infty} P\left( |\bar{X}_n - p| < \varepsilon \right) = 1$，即伯努利大数定律得证。

**考点3：实际应用题**（**结合频率估计概率**）
核心考点
- 利用伯努利大数定律确定“所需试验次数 $n$”（使频率与概率的偏差小于 $\varepsilon$ 的概率不低于 $1-\alpha$）；  
- 解释“统计估计的合理性”（如用样本次品率估计总体次品率）。

典型例题
例3：已知某批产品的次品率 $p = 0.05$，现需抽查 $n$ 件产品，使“次品频率与 $p$ 的偏差小于0.01”的概率不低于0.95，求最小的 $n$。  

**解析**：  
1. 设 $n_A$ 为次品数，需满足 $P\left( \left| \frac{n_A}{n} - 0.05 \right| < 0.01 \right) \geq 0.95$；  
2. 由切比雪夫不等式（伯努利大数定律的推导工具）：  
   $P\left( |\bar{X}_n - p| < \varepsilon \right) \geq 1 - \frac{D(\bar{X}_n)}{\varepsilon^2} = 1 - \frac{p(1-p)}{n\varepsilon^2}$；  
3. 代入 $p=0.05$，$\varepsilon=0.01$，要求 $1 - \frac{0.05 \times 0.95}{n \times (0.01)^2} \geq 0.95$；  
4. 解不等式：  
   $\frac{0.0475}{0.0001n} \leq 0.05 \implies n \geq \frac{0.0475}{0.05 \times 0.0001} = 9500$；  
   故最小 $n$ 为9500。
# 大数定律的辨析（易混考点）

| 定律名称       | 适用随机变量序列       | 核心条件差异                     | 收敛形式       | 地位/作用                     |
|----------------|------------------------|----------------------------------|----------------|-------------------------------|
| 辛钦大数定律   | 独立同分布（i.i.d.）   | 仅需“期望存在”（方差可不存在）   | 依概率收敛     | 最通用，适用于任意i.i.d.序列  |
| 伯努利大数定律 | 独立同分布的0-1分布    | 期望 $p$、方差 $p(1-p)$ 均存在 | 依概率收敛     | 辛钦定律的**特例**（针对伯努利试验） |
| 切比雪夫大数定律 | 相互独立（可不同分布） | 需“期望存在+方差一致有界”         | 依概率收敛     | 适用范围广，但条件更严格       |

*关键考点：判断给定序列适用哪个定律。例如：“设 $X_i$ 独立同分布于柯西分布（期望不存在）”——此时辛钦、伯努利、切比雪夫定律均不适用；“设 $X_i$ 独立同分布于 $U(0,1)$（期望0.5存在，方差1/12存在）”——适用辛钦定律，也可适用切比雪夫定律，但辛钦更简洁。
# 中心极限定理

## (一) [[03数学/03_概率论/05大数定律和中心极限定理/列维-林德伯格中心极限定理\|列维-林德伯格中心极限定理]]
它从理论上解释了 “为何现实中大量随机现象（如身高、体重、考试成绩等）的分布会趋近于正态分布”，是连接 “有限样本” 与 “渐近正态分布” 的关键桥梁，也是后续统计推断（如区间估计、假设检验）的重要理论基础。
设随机变量  $X_{1},X_{2},\dots ,X_{n},\dots$  相互独立,服从相同的分布,具有数学期望  $E(X_{n}) = \mu$  和方差
$D(X_{n}) = \sigma^{2}$  (  $n = 1,2\dots$  ),则对于任意实数  $x$  ,有  $\lim_{n\to \infty}P\left\{\frac{\sum_{k = 1}^{n}X_{k} - n\mu}{\sqrt{n}\sigma}\leqslant x\right\} = \Phi (x).$
![Pasted image 20250916210851.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916210851.png)
![Pasted image 20250916210936.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916210936.png)


## (二) 棣莫弗-拉普拉斯中心极限定理

设随机变量  $X_{n}$  服从参数为  $n$  ,  $p$  的二项分布,即  $X_{n}\sim B(n,p)(0< p< 1,n = 1,2,\dots)$  ,则对于任意实数  $x$  ,有

$$
\lim_{n\to \infty}P\left\{\frac{X_{n} - np}{\sqrt{np(1 - p)}}\leqslant x\right\} = \Phi (x).
$$
![Pasted image 20250916211029.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916211029.png)
![Pasted image 20250916211108.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916211108.png)
![Pasted image 20250916211202.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916211202.png)
![Pasted image 20250916211307.png](/img/user/01%E9%99%84%E4%BB%B6/Pasted%20image%2020250916211307.png)
**良哥解读**
(1) 列维-林德伯格中心极限定理的本质:
在一定条件下(通常指随机变量序列相互独立、同分布、期望、方差均存在),足够多的随机变量的和  $\left(\sum_{k = 1}^{n}X_{k}\right)$  近似服从正态分布,再将其标准化,按照标准正态分布进一步解决问题.
